---
layout: post
title:  "케라스 창시자에게 배우는 딥러닝 "
---

#  신경망과의 첫만남

케라스에서 MNIST 데이터셋 적재하기

from tensorflow.keras.datasets import mnist #mnist 라는 데이터 셋을 사용하겠다는 겁니다.
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11490434/11490434 [==============================] - 1s 0us/step

---------------------

train_images.shape

(60000, 28, 28) # 6만개의 원소가 있다.

---------------------

train_labels
array([5, 0, 4, ..., 5, 6, 8], dtype=uint8) # 각각의 값들이 스칼라 값으로 되어있다.


test_images.shape
(10000, 28, 28) #10000개의 데이터가 있다.

---------------------

# 신경망 구조

from tensorflow import keras
from tensorflow.keras import layers
model = keras.Sequential([             # model 이라고 만든다
    layers.Dense(512, activation="relu"),  # 모든 노드들이 연결되어있는 Dense 형태로 되어있다. 
    layers.Dense(10, activation="softmax")
])




# 컴파일 단계 (학습)

model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
              
              
# 이미지 데이터 준비하기
 
train_images = train_images.reshape((60000, 28 * 28)) # reshape을 통해서 60000, 28 * 28 로 차원을 하나 줄인다.
train_images = train_images.astype("float32") / 255   #normalization 
test_images = test_images.reshape((10000, 28 * 28)) #
test_images = test_images.astype("float32") / 255

# 모델 훈련

model.fit(train_images, train_labels, epochs=5, batch_size=128) # epochs=5는 앞에서 설정해놓은 다섯번 반복하겠다는 것이다.

Epoch 1/6
469/469 [==============================] - 3s 6ms/step - loss: 0.0289 - accuracy: 0.9915
Epoch 2/6
469/469 [==============================] - 2s 3ms/step - loss: 0.0218 - accuracy: 0.9937
Epoch 3/6
469/469 [==============================] - 1s 3ms/step - loss: 0.0161 - accuracy: 0.9955
Epoch 4/6
469/469 [==============================] - 1s 3ms/step - loss: 0.0121 - accuracy: 0.9969
Epoch 5/6
469/469 [==============================] - 1s 3ms/step - loss: 0.0091 - accuracy: 0.9976
Epoch 6/6
469/469 [==============================] - 2s 4ms/step - loss: 0.0064 - accuracy: 0.9987
<keras.callbacks.History at 0x7efd25fed8b0> # 반복하면서 loss가 줄고 accuracy가 늘어나는걸 확인할 수 있다

# 모델을 사용해 예측 만들기

test_digits = test_images[0:10]  #학습에 사용되지 않은 test_images 값을 넣어서 예측할 수 있다.
predictions = model.predict(test_digits)
predictions[0]

1/1 [==============================] - 0s 20ms/step
array([1.28947120e-09, 4.79580785e-11, 7.86466270e-09, 1.17754917e-06,
       5.36497752e-15, 1.80114423e-10, 2.73185304e-16, 9.99998689e-01,
       3.35246247e-10, 1.02023186e-07], dtype=float32)


predictions[0].argmax() # predictions[0] 이 값이 가장 크게 되는 위치 값을 argmax 라고 합니다.
7 

predictions[0][7] # 7일때의 확렬을 나타내준다
0.9999987


test_labels[0] # 실제 레이블과 비교해보면 일치한다.
7




# 새로운 데이터에서 모델 평가하기

test_loss, test_acc = model.evaluate(test_images, test_labels) # 전체 테스트 셋에 대한 확인하는것입니다.  test_labels인 전체 10000개의 데이터에 대해서 체크를 해서 봅니다
print(f"테스트 정확도: {test_acc}")

313/313 [==============================] - 1s 2ms/step - loss: 0.0639 - accuracy: 0.9822
테스트 정확도: 0.982200026512146

- (텐서란 매우 수학적인 개념으로 데이터의 배열이라고 볼 수 있습니다. 텐서의 Rank는 간단히 말해서 몇 차원 배열인가를 의미합니다.)

## 신경망을 위한 데이터 표현


# 스칼라 (랭크-0 텐서)

import numpy as np
x = np.array(12)
x

array(12) # array(12)로 출력된다.



# 벡터 (랭크-1 텐서)
x = np.array([12, 3, 6, 14, 7]) #한 방향으로 숫자가 나열되어 있으면 Dimension1 이다.
x.ndim
x

1 # x.ndim의 값인 1 이 출력된다 
array([12,  3,  6, 14,  7]) # 1차원이다


# 행렬 (랭크-2 텐서)

x = np.array([[5, 78, 2, 34, 0],
              [6, 79, 3, 35, 1],
              [7, 80, 4, 36, 2]])
x.ndim #x 의 x의 Dimension 값 출력

2 # x의 Dimension인 2가 출력된다

# 랭크-3 텐서와 더 높은 랭크의 텐서

x = np.array([[[5, 78, 2, 34, 0],
               [6, 79, 3, 35, 1],
               [7, 80, 4, 36, 2]],
              [[5, 78, 2, 34, 0],
               [6, 79, 3, 35, 1],
               [7, 80, 4, 36, 2]],
              [[5, 78, 2, 34, 0],
               [6, 79, 3, 35, 1],
               [7, 80, 4, 36, 2]]])
x.ndim #x 의 x의 Dimension 값 출력

3 # x의 Dimension인 3이 출력된다

# 핵심 속성

train_images.ndim # 그 값의 dimension을 알려준다

train_images.shape # 그 각각의 dimension에 원소의 갯수를 알 수 있다.

train_images.dtype # 어떤 타입인지 알 수 있습니다.


# 다섯 번째 이미지 출력하기

import matplotlib.pyplot as plt # matplotlib이라는 라이브러리를 사용한다
digit = train_images[4] # 4번째 해당하는 이미지를 불러서
plt.imshow(digit, cmap=plt.cm.binary)
plt.show() # show를 하면 볼 수 있다.

train_labels[4] # 4번째 label의 값을 알려준다.
9 # 값이 9가 나온다


# 넘파이로 텐서 조작하기

my_slice = train_images[10:100] # 슬라이싱 이미지에서 10번째 이미지부터 99번째 이미지까지 가져오겠다는 명령이다. 총 90개
my_slice.shape

(90, 28, 28) 

----------------


# 배치 데이터

batch = train_images[128:256] #원하는 배치만큼 묶어서 만들수도 있습니다.


## 신경망의 톱니바퀴: 텐서 연산

# 원소별 연산

def naive_relu(x): # x가 텐서이다
    assert len(x.shape) == 2 #shape은 2 이다
    x = x.copy()
    for i in range(x.shape[0]): # x의 크기에 맞게 움직인다.
        for j in range(x.shape[1]):
            x[i, j] = max(x[i, j], 0)
    return x
    
------------------


def naive_add(x, y): # x텐서와 y 텐서가 더해질때는 각각에 위치에서 계산이 된다.
    assert len(x.shape) == 2
    assert x.shape == y.shape
    x = x.copy()
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] += y[i, j] # 각각에 위치에서 계산이 된다.
    return x


# 브로드캐스팅 (크기가 다른 텐서도 계산을 할 수 있다)

import numpy as np
X = np.random.random((32, 10)) # 2차원이고
y = np.random.random((10,)) # 1차원이면 덧셈이 되면 안되지만 브로드캐스트를 통해서 덧셈을 할 수 있도록 만들 수 있다.

------------------------------

def naive_add_matrix_and_vector(x, y):
    assert len(x.shape) == 2
    assert len(y.shape) == 1 # y의 차원을 하나 늘린다.
    assert x.shape[1] == y.shape[0] 
    x = x.copy() # copy를 해놓고
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] += y[j] # y에 j가 있다
    return x # x에 들어가는건 


# 텐서 곱셈

x = np.random.random((32,))
y = np.random.random((32,))
z = np.dot(x, y) # 

def naive_vector_dot(x, y): 
    assert len(x.shape) == 1
    assert len(y.shape) == 1
    assert x.shape[0] == y.shape[0]
    z = 0.
    for i in range(x.shape[0]):
        z += x[i] * y[i]
    return z


# 텐서 크기 변환

train_images = train_images.reshape((60000, 28 * 28)) # 60000, 768로 변경 할 수 있습니다.


x = np.array([[0., 1.],
             [2., 3.],
             [4., 5.]])
x.shape # 3x2가 출력된다

(3, 2) 

x = x.reshape((6, 1)) # 6x1로 변경한다
x

--------------

array([[0.],
       [1.],
       [2.],
       [3.],
       [4.],
       [5.]]) # 6x1 로 변경되어서 출력된다.

-----------------

x = x.reshape((2,3)) # 반대로 2,3으로 하면 더이상 우리가 알고 있는 이미지 형태가 아니다. 
x

array([[0., 1., 2.],
       [3., 4., 5.]]) 


------------

x = np.zeros((300, 20)) 
x = np.transpose(x) # 각각의 크기가 반대로 바뀐다
x.shape
(20, 300)

---------------




# 텐서플로의 그레이디언트 테이프

그레이디언트란 변수가 여러개일때 각각의 변수 방향으로 변화율을 확인하는것이다.

import tensorflow as tf
x = tf.Variable(0.)
with tf.GradientTape() as tape: # GradientTape을 통해서 Gradient를 자동으로 계산할 수 있습니다.
    y = 2 * x + 3
grad_of_y_wrt_x = tape.gradient(y, x)



데이터를 만들고 모델을 만든 후 원하는 형태로 학습을 시킨 후 데이터를 주입하여 학습시켜, 학습 결과를 알 수 있다.







