---
layout: post
title:  "케라스 창시자에게 배우는 딥러닝 8장 "
---

##  Convolutional layer 와 fully conneted layer 의 차이점

fully conneted layer

3D -> 1D
: 3차원 데이터를 평평한 1차원 데이터로 평탄화 해줘야 하는데 3차원속에서 의미를 가지는 패턴( 이미지의 RGB, 픽셀 거리사이의 밀접성) 들이 사라지게 된다.
Convolution layer

3D -> 3D
: 이미지 형상을 가진 데이터를 제대로 이해할 가능성이 있다.


# CNN 구조
특징 추출 영역 + 분류
추출 영역 (Feature extraction)

Convolution layer 와 Pooling Layer를 여러겹 쌓는 구조
분류 (Classification)

분류를 위해 Fully conneted layer가 추가된다.


## 추출 영역
# Filter
각 채널에 따른 필터를 적용함으로써 합성곱 계층의 출력 이미지가 생성된다.
4x4x1 -> 2x2x1


# Stride
필터의 이동량
출력 크기에 영향

stride =1  # 필터가 한칸씩 움직이면서 변경되는것입니다.
stried = 2 # 동일하게 두칸씩 움직이면서 변경되는것입니다.


# Padding
합성곱 계층을 거치면서 점차 이미지의 크기가 작아지게 되고, 가장자리 픽셀 정보가 점차 
사라지게 되는 문제를 해결.
가장 자리에 숫자를 추가함으로써 입력과 출력의 크기를 비슷하게 만든다.


# Pooling
앞선 방법으로 이미지의 크기를 계속 유지한 채로 
fully connected layer로 가게 된다면 연상량이 크기 때문에 적당히 크기도 줄이고, 
특정 feature를 강조 할수 있어야함
종류 3가지 : max, average, min
CNN 에서는 주로 Max pooling 사용

# 분류
Fully connected layer
Flatten layer

데이터 타입을 fully connected 네트워크 형태로 변경
Softmax layer

Classification 수행

## 합성곱 신경망 소개

# 간단한 컨브넷 만들기

from tensorflow import keras # keras를 불러옵니다.
from tensorflow.keras import layers # 케라스에서 layers를 쌓는다고 명령합니다.
inputs = keras.Input(shape=(28, 28, 1))
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs) #필터의 사이즈는 3x3 으로 합니다
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x) 
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(10, activation="softmax")(x) # 내가 원하는 클래스의 갯수만큼 출력합니다.
model = keras.Model(inputs=inputs, outputs=outputs)


# 모델의 summary() 메서드 출력
model.summary()


Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 28, 28, 1)]       0         #처음에는 28, 28입니다
                                                                 
 conv2d (Conv2D)             (None, 26, 26, 32)        320       # 32개로 증가합니다
                                                                 
 max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         # 13,13 으로 변화합니다
 )                                                               
                                                                 
 conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     #11,11로 변화합니다
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         #5 , 5로 변화합니다
 2D)                                                             
                                                                 
 conv2d_2 (Conv2D)           (None, 3, 3, 128)         73856     #conv2d에 들어가면 3,3 으로 변경되빈다.
                                                                 
 flatten (Flatten)           (None, 1152)              0         #노드가 1152개로 변경됩니다
                                                                 
 dense (Dense)               (None, 10)                11530     # 1152개를 10개 있으므로 11530개가 필요합니다.
                                                                 
=================================================================
Total params: 104,202
Trainable params: 104,202
Non-trainable params: 0


# MNIST 이미지에서 컨브넷 훈련하기

from tensorflow.keras.datasets import mnist

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.reshape((60000, 28, 28, 1)) 
train_images = train_images.astype("float32") / 255 # 이미지 밝기가 255인것을 한번 나눠서 0과 1 사이가 되도록 변경하는것
test_images = test_images.reshape((10000, 28, 28, 1))
test_images = test_images.astype("float32") / 255
model.compile(optimizer="rmsprop",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"])
model.fit(train_images, train_labels, epochs=5, batch_size=64) # 모델에 변경시킨 데이터를 넣고 반복 5번을 합니다.


Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11490434/11490434 [==============================] - 2s 0us/step
Epoch 1/5
938/938 [==============================] - 17s 4ms/step - loss: 0.1641 - accuracy: 0.9498
Epoch 2/5
938/938 [==============================] - 4s 4ms/step - loss: 0.0449 - accuracy: 0.9857
Epoch 3/5
938/938 [==============================] - 4s 5ms/step - loss: 0.0314 - accuracy: 0.9903
Epoch 4/5
938/938 [==============================] - 4s 4ms/step - loss: 0.0235 - accuracy: 0.9926
Epoch 5/5
938/938 [==============================] - 4s 4ms/step - loss: 0.0176 - accuracy: 0.9945
<keras.callbacks.History at 0x7fb73c5f0c40> # 총 다섯번을 반복 하여 loss를 줄입니다.

## 최대 풀링 연산

# 최대 풀링 층이 빠진 잘못된 구조의 컨브넷

inputs = keras.Input(shape=(28, 28, 1))
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(10, activation="softmax")(x)
model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)


# maxpooling 하지 않을 경우 문제
# 1. 학습시키는데 도움이 되지 않는다
# 2. 61952 처럼 너무 많은 feature map이 나오게된다. 즉 가중치가 너무 쎄서 overfiiting된다.


## 소규모 데이터셋에서 밑바닥부터 컨브넷 훈련하기

# 작은 데이터셋 문제에서 딥러닝의 타당성

캐글에서 dogs-vs-cats 데이터셋을 다운로드하려면 캐글에 가입해야 한 후 생성한 API 키를 사용해야 합니다. 
이런 과정이 번거롭다면 다음 명령으로 구글 드라이브에서 직접 다운로드할 수 있습니다.


# 이미지를 훈련, 검증, 테스트 디렉토리로 복사하기

import os, shutil, pathlib

original_dir = pathlib.Path("train")                                #원본 데이터셋이 압축 해제 되어있는 디렉토리 경로
new_base_dir = pathlib.Path("cats_vs_dogs_small")                   #서브셋 데이터를 저장할 디렉토리
 
def make_subset(subset_name, start_index, end_index):
    for category in ("cat", "dog"):   
        dir = new_base_dir / subset_name / category
        os.makedirs(dir)
        fnames = [f"{category}.{i}.jpg" for i in range(start_index, end_index)]
        for fname in fnames:
            shutil.copyfile(src=original_dir / fname,
                            dst=dir / fname)

make_subset("train", start_index=0, end_index=1000)                 # 처음 1000개의 이미지를 훈련 서브셋으로 만든다.
make_subset("validation", start_index=1000, end_index=1500)         # 1000번부터 1500번까지 검증 서브셋으로 만든다. 
make_subset("test", start_index=1500, end_index=2500)               # 1500번 부터 2500번까지 테스트 서브셋으로 만든다.



## 모델 만들기

# 강아지 vs. 고양이 분류를 위한 소규모 컨브넷 만들기

from tensorflow import keras # 케라스의 레이어
from tensorflow.keras import layers

inputs = keras.Input(shape=(180, 180, 3))            # 180 x 180  RGB 이미지지
x = layers.Rescaling(1./255)(inputs)                 # 255로 나누어 [0,1] 범위로 스케일 조정
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x) # 2x2 사이즈로 변경되어있다
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x) 
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(1, activation="sigmoid")(x) # sigmid 클래스가 두개 이상일때는 softmax, 0과1 일때는 sigmid
model = keras.Model(inputs=inputs, outputs=outputs)


--------------------
model.summary()

Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 180, 180, 3)]     0         
                                                                 
 rescaling (Rescaling)       (None, 180, 180, 3)       0         
                                                                 
 conv2d_6 (Conv2D)           (None, 178, 178, 32)      896       
                                                                 
 max_pooling2d_2 (MaxPooling  (None, 89, 89, 32)       0         
 2D)                                                             
                                                                 
 conv2d_7 (Conv2D)           (None, 87, 87, 64)        18496     
                                                                 
 max_pooling2d_3 (MaxPooling  (None, 43, 43, 64)       0         
 2D)                                                             
                                                                 
 conv2d_8 (Conv2D)           (None, 41, 41, 128)       73856     
                                                                 
 max_pooling2d_4 (MaxPooling  (None, 20, 20, 128)      0         
 2D)                                                             
                                                                 
 conv2d_9 (Conv2D)           (None, 18, 18, 256)       295168    
                                                                 
 max_pooling2d_5 (MaxPooling  (None, 9, 9, 256)        0         
 2D)                                                             
                                                                 
 conv2d_10 (Conv2D)          (None, 7, 7, 256)         590080    
                                                                 
 flatten_2 (Flatten)         (None, 12544)             0         
                                                                 
 dense_2 (Dense)             (None, 1)                 12545     # 12544개의 노드를 1 개로 만드는것 입니다.
                                                                 
=================================================================
Total params: 991,041
Trainable params: 991,041
Non-trainable params: 0
________________________________________________________________

# 모델 훈련 설정하기

model.compile(loss="binary_crossentropy", # 두개의 클래스이기 때문에 binary를 사용합니다.
              optimizer="rmsprop",
              metrics=["accuracy"])



# 데이터 전처리

image_dataset_from_directory를 사용하여 이미지 읽기

사진 파일을 읽습니다
jpeg 콘테츠를 RGB 픽셀값으로 디코딩
그다음 부동 소수점 타입의 텐서로 변환
동일한 크기의 이미지로 바꿉니다 (180 x 180)
배치로 묶습니다 (1개의 배치 = 32개 이미지) #나중에 업데이트를 위해서 배치 단위로 변경



image_dataset_from_directory 를 통해서 케라스에서는 자동으로 처리리
from tensorflow.keras.utils import image_dataset_from_directory # image_dataset_from_directory이용합니다.

train_dataset = image_dataset_from_directory(
    new_base_dir / "train",
    image_size=(180, 180),
    batch_size=32)
validation_dataset = image_dataset_from_directory(
    new_base_dir / "validation",
    image_size=(180, 180),
    batch_size=32)
test_dataset = image_dataset_from_directory(
    new_base_dir / "test",
    image_size=(180, 180),
    batch_size=32)

# Dataset이 반환하는 데이터와 레이블 크기 확인하기

for data_batch, labels_batch in train_dataset:
    print("데이터 배치 크기:", data_batch.shape)
    print("레이블 배치 크기:", labels_batch.shape)  # (180, 180)으로 되어있습니다
    break


# Dataset을 사용해 모델 훈련하기

callbacks = [
    keras.callbacks.ModelCheckpoint(              # ModelCheckpoint 콜백을 사용하여  epoch가 끝날때마다 모델을 저장하며  
        filepath="convnet_from_scratch.keras",    # 저장되는 경로와 매개변수가 각각 save_best_only와 moitor
        save_best_only=True,                      # 검증 데이터 성능이 가장 좋은 모델이 저장되어 과대적합까지 할 필요가 사라짐  
        monitor="val_loss")
]
history = model.fit(                                           # validation_data 맥변수를 사용하여 별도의 dataset 객체로 검증 지표 모니터링 
    train_dataset,
    epochs=30,
    validation_data=validation_dataset,
    callbacks=callbacks)

# 훈련 정확도와 손실 그래프 그리기

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "bo", label="Training accuracy")
plt.plot(epochs, val_accuracy, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()

결과로는 Training accuracy는 높아지는걸 확인할 수 있지만 
Validation accuracy은 위아래로 변화가 심하다는걸 알 수 있습니다.

epoch 9번 정도 만에 검증 손실이 최소가 되었다가 향상되지 않습니다
검증 정확도는 75%가 최고입니다.

# 테스트 세트에서 모델 평가하기
test_model = keras.models.load_model("convnet_from_scratch.keras") # 저장되어 있는 model을 불러옵니다.
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"테스트 정확도: {test_acc:.3f}")

테스트 결과로는 0.700으로 나타나는걸 알 수 있습니다.

# 데이터 증식 사용하기

과대적합이 학습할 샘플이 너무 적어 새로운 데이터에 일반화할수 있는 모델을 훈련시킬수 없기 때문에 
발생하기 때문에 이를 위해 많은 데이터가 주어질 필요가 있고, 
데이터 증식을 통해 기존 훈련 샘플로부터 더 많은 훈련 데이터를 생성하는 방법을 이용할 것 이다.

컨브넷에 추가할 데이터 증식 단계 정의하기

data_augmentation = keras.Sequential(                     #데이터 증식 층 (data_augmentation_layer)을 추가 하는 방법법
    [
        layers.RandomFlip("horizontal"),                  # 랜덤하게 50% 이미지를 수평으로 뒤집는다  
        layers.RandomRotation(0.1),                       # -10% ~ 10% 중 랜덤하게 입력이미지를 회전 시킨다  
        layers.RandomZoom(0.2),                           # -20% ~ 20% 중 랜덤하게 입력이미지를 확대 혹은 축소 시킨다  
   ]
)

# 랜덤하게 증식된 훈련 이미지 출력하기

plt.figure(figsize=(10, 10))
for images, _ in train_dataset.take(1):
    for i in range(9): # for 문을 사용하여 반복합니다.
        augmented_images = data_augmentation(images) # augmentation 이미지를 만들겠다는 명령입니다.
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
        
새로운 모델을 훈련 시킬 때 같은 데이터가 사용되지 않지만
기존 이미지를 변환한것 이기 때문에 상호 연관성이 크다.
따라서 과대적합을 더 막아주기 위해서 밀집 연결 분류기 직전에 Dropout층을 추가하겠다.


# 이미지 증식과 드롭아웃을 포함한 컨브넷 만들기


inputs = keras.Input(shape=(180, 180, 3))
x = data_augmentation(inputs)                                                # 이미지 증식
x = layers.Rescaling(1./255)(x)
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
x = layers.Dropout(0.5)(x)                                                   # dropout 사용
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

model.compile(loss="binary_crossentropy",
              optimizer="rmsprop",
              metrics=["accuracy"])


## 사전 훈련된 모델 활용하기

# 사전 훈련된 모델

일반적으로 대규모 이미지 분류 문제를 위해 대량의 데이터 셋에서 미리 훈련된 모델

작은 이미지 데이터셋에 딥러닝을 적용하는 일반적이고 효과적인 방법

특성 추출 과 미세 조정 을 이용하여 사전 훈련된 모델을 사용 한다

## 사전 훈련된 모델을 사용한 특성 추출

# 특성 추출
사전에 학습된 모델의 표현을 사용하여 새로운 샘플에서 흥미로운 특성을 뽑아내는 것

이러한 특성을 사용하여 새로운 분류기를 처음 부터 훈렵시킨다

# VGG16 합성곱 기반 층 만들기
(16개의 층을 갖고 있습니다)
conv_base = keras.applications.vgg16.VGG16(    # 케라스에 패키지로 포함되어 있는 VGG16 모델 사용
    weights="imagenet",                # weights : 모델을 초기화할 가중치 체크포인트 지정  
    include_top=False,               # include_top : 네트워크 맨 위에 놓인 밀집 연결 분류기를 포함 할지 안할지 지정 
                                   # 이번 경우에는 강와지와 고양이 2개의 클래스를 구분하는 층을 추가하려고 함으로 이를 포함시키지 않는다  
    input_shape=(180, 180, 3)) 

-------------------
conv_base.summary() # 이미 학습 되어 있는 모델을 확인합니다.

# 밀집 연결 분류기 정의하고 훈련하기

inputs = keras.Input(shape=(5, 5, 512))
x = layers.Flatten()(inputs)                        
x = layers.Dense(256)(x)                                  
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)         
model = keras.Model(inputs, outputs)                      
model.compile(loss="binary_crossentropy",
              optimizer="rmsprop",
              metrics=["accuracy"])

callbacks = [
    keras.callbacks.ModelCheckpoint(
      filepath="feature_extraction.keras",
      save_best_only=True,
      monitor="val_loss")
]
history = model.fit(
    train_features, train_labels,
    epochs=20,
    validation_data=(val_features, val_labels),
    callbacks=callbacks)
    
    
# 결과를 그래프로 나타내기    
    
import matplotlib.pyplot as plt
acc = history.history["accuracy"]
val_acc = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, "bo", label="Training accuracy")
plt.plot(epochs, val_acc, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()


test_model = keras.models.load_model(
    "feature_extraction.keras")
test_loss, test_acc = test_model.evaluate(test_features,test_labels)
print(f"테스트 정확도: {test_acc:.3f}")

dropout을 사용하고 훈련을 했음에도 빠른시간에 과대적합이 되었다 
왜냐하면 작은 이미지 데이터셋에 필수 적인 데이터 증식을 하지 않았기 때문입니다.

## 사전 훈련된 모델 미세 조정하기


# 미세조정

특성 추출에 사용했던 동결 모델의 상위 층 몇개를 동결에서 
해제하고 모델에 새로 추가한 층과 함께 훈련 하는 것

# 미세조정 단계

사전에 훈련된 기반 네트워크 위에 새로운 네트워크를 추가
기반 네트워크 동결합니다.
새로 추가한 네트워크를 훈련합니다
기반 네트워크에서 일부 층의 동결 해제합니다
동결을 해제한 층과 새로 추가한 층을 함께 훈련 합니다

1~3번은 앞의 과정을 통해 완료함. 4번부터 실시합니다


conv_base.trainable = True      #위에서부터 block4_pool 까지 동결 , block5_conv 1,2,3 층은 학습 대상  
for layer in conv_base.layers[:-4]: 
    layer.trainable = False


# 모델 미세 조정하기


model.compile(loss="binary_crossentropy",
              optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),         # 학습률을 낮추어 미세 조정하는 3개의 층에서 학습된 표현을 조금씩 수정하도록한다.
                                                                              # 너무 변경량이 크게되면 학습된 표현에 나쁜 영향을 끼칠 수 있다    
              metrics=["accuracy"])

callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="fine_tuning.keras",
        save_best_only=True,
        monitor="val_loss")
]
history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=validation_dataset,
    callbacks=callbacks)
    
  ------------------------------
  
model = keras.models.load_model("fine_tuning.keras")
test_loss, test_acc = model.evaluate(test_dataset)
print(f"테스트 정확도: {test_acc:.3f}") #테스트를 해보면 정확도가 늘어난것을 확인할 수 있습니다.

# 많은 층을 미세 조정 안하는 이유
하위층 : 일반적이고 재사용 가능한 특성
상위층 : 구체적인 특성
따라서 새로운 문제에 재활용 될 수 있도록 수정이 필요한 특성이 구체적인 특성들이기 때문에 주로 상위 층 몇개만 미세 조정하는것이 유리

훈련해야할 파라미터가 늘어날 수록 과대적합 위험


# 요약
작은 데이터셋에서도 처음 부터 훈련해서 괜찮은 성능을 낸다
CNN은 패턴과 개념의 계층 구조를 학습한다
작은 데이터셋의 과대적합을 막기 위해 데이터증식을 사용한다
특성 추출 방식으로 기존의 convnet을 쉽게 재사용 가능 (이미 학습되어 있는 모델을 확인하고)
미세조정을 통해 특성 추출을 보완 가능










