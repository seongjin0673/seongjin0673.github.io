---
layout: post
title:  "케라스 창시자에게 배우는 딥러닝 4장 "
---

##  신경망 시작하기: 분류와 회귀

분류에는 3가지가 있습니다. 

긍정 부정을 하는 2진 분류

뉴스기사 분류 하는 :다중분류

주택 가격을 예측하는 회귀 문제

# 2진 분류 문제
# IMDB 데이터셋

IMDB 데이터셋 로드하기

from tensorflow.keras.datasets import imdb  # imdb라는 데이터를 import해서 불러옵니다.
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(  
    num_words=10000)
    
    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz
17464789/17464789 [==============================] - 0s 0us/step


train_data[0] #train data 0 번에 있는 값을 출력합니다.
[1,
 14,
 22,
 16,
 43,
 530,
 973,
 1622,
 1385,
 65,
출력은 각 단어들에 대한 연결된 숫자가 나타납니다.


train_labels[0] # 긍정인지 부정인지 출력해준다.

max([max(sequence) for sequence in train_data]) #train data는 2만 5천자 0부터 9999숫자 까지 

리뷰를 다시 텍스트로 디코딩하기
word_index = imdb.get_word_index()
reverse_word_index = dict(
    [(value, key) for (key, value) in word_index.items()])
decoded_review = " ".join(
    [reverse_word_index.get(i - 3, "?") for i in train_data[0]])

## 데이터 준비

# 정수 시퀀스를 멀티-핫 인코딩으로 인코딩하기 (시퀀스를 인코딩 하는 방법)

import numpy as np
def vectorize_sequences(sequences, dimension=10000):#0부터 9999까지 만들고 
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        for j in sequence:
            results[i, j] = 1. 
    return results
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

----------------------------------------------

x_train[0] #단어가 있는 위치에만 1이 있고 나머지는 0


from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(16, activation="relu"), # Dense란 10000개를 16개로 만듭니다
    layers.Dense(16, activation="relu"), # 16개를 16개로 # 16개에서 16개로 만듭니다
    layers.Dense(1, activation="sigmoid") # 0,1 레인지가 되도록 만듭니다.
])

------------------------------

model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])


# 훈련 검증
# 검증 세트 준비하기

x_val = x_train[:10000]
partial_x_train = x_train[10000:] # x_train
y_val = y_train[:10000]
partial_y_train = y_train[10000:]


history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,  # 20번 반복을 합니다
                    batch_size=512, #512개씩 묶은  batch 단위로 전체 2만 5천개를 나눠서 교육을 합니다.
                    validation_data=(x_val, y_val))



Epoch 1/20
30/30 [==============================] - 3s 61ms/step - loss: 0.5191 - accuracy: 0.7868 - val_loss: 0.3952 - val_accuracy: 0.8657
Epoch 2/20
30/30 [==============================] - 1s 36ms/step - loss: 0.3246 - accuracy: 0.8939 - val_loss: 0.3206 - val_accuracy: 0.8791
Epoch 3/20
30/30 [==============================] - 1s 36ms/step - loss: 0.2447 - accuracy: 0.9185 - val_loss: 0.3061 - val_accuracy: 0.8766
Epoch 4/20
30/30 [==============================] - 1s 36ms/step - loss: 0.2019 - accuracy: 0.9325 - val_loss: 0.2744 - val_accuracy: 0.8895
Epoch 5/20
30/30 [==============================] - 1s 37ms/step - loss: 0.1678 - accuracy: 0.9455 - val_loss: 0.2846 - val_accuracy: 0.8853
Epoch 6/20
30/30 [==============================] - 1s 35ms/step - loss: 0.1440 - accuracy: 0.9537 - val_loss: 0.2815 - val_accuracy: 0.8863
Epoch 7/20
30/30 [==============================] - 1s 36ms/step - loss: 0.1234 - accuracy: 0.9609 - val_loss: 0.3201 - val_accuracy: 0.8760
Epoch 8/20
30/30 [==============================] - 2s 56ms/step - loss: 0.1045 - accuracy: 0.9693 - val_loss: 0.3152 - val_accuracy: 0.8794
Epoch 9/20
30/30 [==============================] - 1s 50ms/step - loss: 0.0910 - accuracy: 0.9735 - val_loss: 0.3182 - val_accuracy: 0.8829
Epoch 10/20
30/30 [==============================] - 1s 36ms/step - loss: 0.0761 - accuracy: 0.9794 - val_loss: 0.3510 - val_accuracy: 0.8749
Epoch 11/20
30/30 [==============================] - 1s 35ms/step - loss: 0.0687 - accuracy: 0.9811 - val_loss: 0.3674 - val_accuracy: 0.8743
Epoch 12/20
30/30 [==============================] - 1s 37ms/step - loss: 0.0546 - accuracy: 0.9881 - val_loss: 0.3715 - val_accuracy: 0.8786
Epoch 13/20
30/30 [==============================] - 1s 35ms/step - loss: 0.0465 - accuracy: 0.9899 - val_loss: 0.3949 - val_accuracy: 0.8763
Epoch 14/20
30/30 [==============================] - 1s 35ms/step - loss: 0.0419 - accuracy: 0.9904 - val_loss: 0.4089 - val_accuracy: 0.8772
Epoch 15/20
30/30 [==============================] - 1s 35ms/step - loss: 0.0331 - accuracy: 0.9936 - val_loss: 0.4433 - val_accuracy: 0.8720
Epoch 16/20
30/30 [==============================] - 1s 35ms/step - loss: 0.0276 - accuracy: 0.9957 - val_loss: 0.5038 - val_accuracy: 0.8624
Epoch 17/20
30/30 [==============================] - 1s 34ms/step - loss: 0.0239 - accuracy: 0.9967 - val_loss: 0.4735 - val_accuracy: 0.8730
Epoch 18/20
30/30 [==============================] - 1s 36ms/step - loss: 0.0196 - accuracy: 0.9973 - val_loss: 0.5020 - val_accuracy: 0.8711
Epoch 19/20
30/30 [==============================] - 1s 47ms/step - loss: 0.0186 - accuracy: 0.9967 - val_loss: 0.5193 - val_accuracy: 0.8726
Epoch 20/20
30/30 [==============================] - 2s 60ms/step - loss: 0.0101 - accuracy: 0.9996 - val_loss: 0.5467 - val_accuracy: 0.8721

결과값을 보면 0.7에서 0.9999까지 accuracy가 높아집니다.


# 훈련과 검증 손실 그리기

import matplotlib.pyplot as plt
history_dict = history.history
loss_values = history_dict["loss"]
val_loss_values = history_dict["val_loss"]
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

epochs를 확인해보면 4 이후부터 loss가 줄어들었지만 , validation은 늘어나는걸 확인할 수 있습니다.
동일한 문제에 대해서 학습을 하다보니, 동일한 문제만 정확하게 풀 수 있는걸 확인할 수 있습니다.


# 모델을 처음부터 다시 훈련하기
model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
model.fit(x_train, y_train, epochs=4, batch_size=512) #4에서 멈추고 
results = model.evaluate(x_test, y_test)

validation이 늘어나지 않는 단계인 epochs 4로 설정을 한 후에 다시 훈련을 시켜줍니다.

Epoch 1/4
49/49 [==============================] - 2s 27ms/step - loss: 0.5016 - accuracy: 0.7912
Epoch 2/4
49/49 [==============================] - 2s 32ms/step - loss: 0.2922 - accuracy: 0.8980
Epoch 3/4
49/49 [==============================] - 2s 39ms/step - loss: 0.2247 - accuracy: 0.9191
Epoch 4/4
49/49 [==============================] - 1s 29ms/step - loss: 0.1900 - accuracy: 0.9314
782/782 [==============================] - 2s 2ms/step - loss: 0.2969 - accuracy: 0.8807

그 결과 loss와 accuracy가 줄어든것을 확인할 수 있습니다.


# 뉴스 기사 분류: 다중 분류 문제

로이터 데이터셋 로드하기
from tensorflow.keras.datasets import reuters # reuters 데이터를 불러옵니다
(train_data, train_labels), (test_data, test_labels) = reuters.load_data(
    num_words=10000) # 데이터를 10000개 사용하겠다고 명령합니다.
    
   
# 데이터 인코딩하기   
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
   
   
# 레이블 인코딩하기
 
  def to_one_hot(labels, dimension=46):
    results = np.zeros((len(labels), dimension))
    for i, label in enumerate(labels):
        results[i, label] = 1.
    return results
y_train = to_one_hot(train_labels) # one_hot으로 변경합니다.
y_test = to_one_hot(test_labels)


from tensorflow.keras.utils import to_categorical 
y_train = to_categorical(train_labels)
y_test = to_categorical(test_labels)


# 모델 정의하기


model = keras.Sequential([
    layers.Dense(64, activation="relu"),
    layers.Dense(64, activation="relu"),
    layers.Dense(46, activation="softmax") # 누가 확률이 제일 높은지 확인할 수 있는 softmax를 사용합니다.
])

# 모델 컴파일하기

model.compile(optimizer="rmsprop",
              loss="categorical_crossentropy",
              metrics=["accuracy"])
             
동일하게 모델을 컴파일 합니다.



# 훈련 검증

# 검증 세트 준비하기
우선 검증을 하기 위해선 준비를 해야합니다.
x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = y_train[:1000]
partial_y_train = y_train[1000:]

# 모델 훈련하기

history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20, # 20번 반복되도록 설정을 합니다.
                    batch_size=512,
                    validation_data=(x_val, y_val))


----------------------------

# 훈련과 검증 손실 그리기

loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

epochs를 확인해보면 9에서부터 validation이 상승하는걸 확인할 수 있습니다.

그 전에 훈련을 끝내기 위해서  epochs 를 9로 변경합니다.
model = keras.Sequential([
  layers.Dense(64, activation="relu"),
  layers.Dense(64, activation="relu"),
  layers.Dense(46, activation="softmax")
])
model.compile(optimizer="rmsprop",
              loss="categorical_crossentropy",
              metrics=["accuracy"])
model.fit(x_train,
          y_train,
          epochs=9, # epochs 를 9로 변경
          batch_size=512)
results = model.evaluate(x_test, y_test)


------------------------


## 주택 가격 예측: 회귀 문제
 
 어떠한 레이블로 해서 클래스를 나누는게 아닌 값을 실수값으로 예측을 하는것입니다.
 
 # 보스턴 주택 가격 데이터셋


# 보스턴 주택 데이터셋 로드하기

from tensorflow.keras.datasets import boston_housing # 보스턴 집 가격 데이터를 불러옵니다
(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()


train_data.shape #데이터 404개 있는데 13개의 특정 데이터가 있다.

test_data.shape #13개의 데이터가 있는데, 테스트에 사용할 102개의 데이터가 있다. 

(실질적인 데이터는 적기 때문에 실질적으로 원하는 결과를 못얻을 수  있습니다)



# 데이터 준비

# 데이터 정규화하기


mean = train_data.mean(axis=0)
train_data -= mean
std = train_data.std(axis=0)
train_data /= std
test_data -= mean
test_data /= std

# 모델 정의하기
def build_model():
    model = keras.Sequential([
        layers.Dense(64, activation="relu"),
        layers.Dense(64, activation="relu"),
        layers.Dense(1)
    ])
    model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
    return model
activation 을 통과하지 않고 바로 출력시킵니다.




# K-겹 검증하기

k = 4 # 그룹을 4개로 만듭니다.
num_val_samples = len(train_data) // k
num_epochs = 100
all_scores = []
for i in range(k):
    print(f"#{i}번째 폴드 처리중")
    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]
    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]
    partial_train_data = np.concatenate(
        [train_data[:i * num_val_samples],
         train_data[(i + 1) * num_val_samples:]],
        axis=0)
    partial_train_targets = np.concatenate(
        [train_targets[:i * num_val_samples],
         train_targets[(i + 1) * num_val_samples:]],
        axis=0)
    model = build_model()
    model.fit(partial_train_data, partial_train_targets,
              epochs=num_epochs, batch_size=16, verbose=0)
    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
    all_scores.append(val_mae)


#0번째 폴드 처리중
#1번째 폴드 처리중
#2번째 폴드 처리중
#3번째 폴드 처리중
4번에 걸쳐서 처리된 값들이 나타납니다.

# K-겹 검증 점수 평균을 기록하기
average_mae_history = [
    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]
    
    

# 검증 점수 그래프 그리기

plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)
plt.xlabel("Epochs")
plt.ylabel("Validation MAE")
plt.show()

결과값을 보면 앞에 부분이 너무 크게 변화가 되었기 때문에, 뒷부분의 상승되는 결과를 정확히 확인하기 어렵습니다.
그러므로 앞에 있는 부분을 빼고 뒷부분만 다시 한번 그래프로 출력해야합니다.

# 처음 10개의 데이터 포인트를 제외한 검증 점수 그래프 그리기
truncated_mae_history = average_mae_history[10:] # 앞에 있는 부분은 제외하고 10번째부터 검증할 수 있는 그래프를 나타냅니다.
plt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)
plt.xlabel("Epochs")
plt.ylabel("Validation MAE")
plt.show()


# 새로운 데이터에 대해 예측하기

predictions = model.predict(test_data)
predictions[0] 

마지막에는 항상 새로운 데이터가 있으면 predict해서 어떤값이 나올수가 있는지 예측할 수 있다.











