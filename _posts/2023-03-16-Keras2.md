---
layout: post
title:  "케라스 창시자에게 배우는 딥러닝2 "
---

## 케라스와 텐서플로 소개

상수 텐서와 변수

# 모두 1 또는 모두 0인 텐서

import tensorflow as tf
x = tf.ones(shape=(2, 1))
print(x)

tf.Tensor(
[[1.]
 [1.]], shape=(2, 1), dtype=float32)
 
 # 랜덤 텐서
 x = tf.random.normal(shape=(3, 1), mean=0., stddev=1.) #normal을 통해서 원하는 크기로 형성 할 수 있습니다. 
print(x)


# 넘파이 배열에 값 할당하기

import numpy as np
x = np.ones(shape=(2, 2))
x[0, 0] = 0. # numpy에서는 값을 할당 가능 


# 텐서플로 변수 만들기

v = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))
print(v)

<tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy=
array([[ 0.6215516],
       [-0.7677283],
       [-1.0501245]], dtype=float32)>
       
-----------------------------------------

v.assign(tf.ones((3, 1))) #원하는곳에 값을 주기 위해서는 assign을 해야한다.

<tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
array([[1.],
       [1.],
       [1.]], dtype=float32)>
       
       
## 텐서 연산: 텐서플로에서 수학 계산하기


# 기본적인 수학 연산

a = tf.ones((2, 2))
b = tf.square(a) # 제곱
c = tf.sqrt(a) #루트 
d = b + c # 덧셈
e = tf.matmul(a, b) 
e *= d 

## GradientTape API 다시 살펴 보기

# GradientTape 사용하기

input_var = tf.Variable(initial_value=3.)
with tf.GradientTape() as tape:
   result = tf.square(input_var) 
gradient = tape.gradient(result, input_var)

# 그레이디언트 테이프를 중첩하여 이계도 그레이디언트를 계산하기

time = tf.Variable(0.)
with tf.GradientTape() as outer_tape: # 다른 이름으로 지정 outer_tape,inner_tape
    with tf.GradientTape() as inner_tape: # 다른 이름으로 지정 outer_tape,inner_tape
        position =  4.9 * time ** 2 # position을 정의 (position을 time에 미분한 것이다)
    speed = inner_tape.gradient(position, time) # inner에 대해서 먼저 계산한다
acceleration = outer_tape.gradient(speed, time)

## 엔드-투-엔드 예제: 텐서플로 선형 분류기

# 2D 평면에 두 클래스의 랜덤한 포인트 생성하기

num_samples_per_class = 1000
negative_samples = np.random.multivariate_normal(
    mean=[0, 3], #0,3을 기준으로 점들을 1000개 그린다
    cov=[[1, 0.5],[0.5, 1]],
    size=num_samples_per_class)
positive_samples = np.random.multivariate_normal(
    mean=[3, 0],# 3,0 을 기준으로 점들을 1000개 그린다
    cov=[[1, 0.5],[0.5, 1]],
    size=num_samples_per_class)

# 선형 분류기의 변수 만들기

input_dim = 2
output_dim = 1
W = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim)))
b = tf.Variable(initial_value=tf.zeros(shape=(output_dim,)))

# 정방향 패스 함수

def model(inputs):
    return tf.matmul(inputs, W) + b

# 훈련 스텝 함수

learning_rate = 0.1

def training_step(inputs, targets): # update 시키는걸 training 함수로 정의를 시킨다.
    with tf.GradientTape() as tape:
        predictions = model(inputs)
        loss = square_loss(targets, predictions)
    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])
    W.assign_sub(grad_loss_wrt_W * learning_rate)
    b.assign_sub(grad_loss_wrt_b * learning_rate)
    return loss


## 신경망의 구조: 핵심 Keras API 이해하기

# Layer의 서브클래스로 구현한 Dense 층 
인풋의 갯수가 있고 내가 원하는 출력이 있으면 그것만큼 dense 하여 출력한다.
from tensorflow import keras

class SimpleDense(keras.layers.Layer):

    def __init__(self, units, activation=None):
        super().__init__()
        self.units = units
        self.activation = activation

    def build(self, input_shape):
        input_dim = input_shape[-1]
        self.W = self.add_weight(shape=(input_dim, self.units), # W를 학습합니다
                                 initializer="random_normal")
        self.b = self.add_weight(shape=(self.units,),  # b를 학습합니다.
                                 initializer="zeros")

    def call(self, inputs):
        y = tf.matmul(inputs, self.W) + self.b
        if self.activation is not None:
            y = self.activation(y)
        return y


my_dense = SimpleDense(units=32, activation=tf.nn.relu) #  # 32개로 변경시키는것입니다.
input_tensor = tf.ones(shape=(2, 784)) # 인풋이 784개면 그것을 units 인 32개로 내보내는것 입니다.
output_tensor = my_dense(input_tensor) #인풋과 같은 크기로 만드는 레이어 입니다.
print(output_tensor.shape)

# 자동 크기 추론: 동적으로 층 만들기

from tensorflow.keras import layers
layer = layers.Dense(32, activation="relu") #레이어를 32개로 쌓는것 입니다.

from tensorflow.keras import models
from tensorflow.keras import layers
model = models.Sequential([
    layers.Dense(32, activation="relu"), #32개로 만들어서 내보내는 것입니다.
    layers.Dense(32) # 동일하게 32개로 쌓는것 입니다.
])

# “컴파일” 단계: 학습 과정 설정

model = keras.Sequential([keras.layers.Dense(1)])
model.compile(optimizer="rmsprop",
              loss="mean_squared_error",
              metrics=["accuracy"])

-------------------------

history = model.fit( # fit을 통해서 학습이 돌아갑니다.
    inputs,
    targets,
    epochs=5,
    batch_size=128
)


history.history # 학습된 결과를 출력할 수 있습니다.





